# Either "cpu" or "cuda"
# NOTE: Cuda requires enough VRAM to load 3 FP16 models (~45 GB for Mistral)
# NOTE 2: The (much slower) CPU mode still requires Cuda capability, but only enough VRAM to load a model once. (~15 GB for Mistral)
device: "cuda"
random_seed: 42 # Random seed to use

directories:
  model_path1: "/disk/merge-monster/models/Nous-Hermes-2-Mixtral-8x7B-DPO" # Path to the base model. Must be a local copy.
  model_directory: "/disk/merge-monster/models/" # Directory of models to scan, IGNORED if models_to_merge has entries in it
  output_directory: "/disk/merge-monster/output" # Output directory of the merged model

# A list of models to use as merge candidates - HF syntax, so can be either local directories or repos.
# Overrides model_directory if used
models_to_merge: ["ycros/BagelMIsteryTour-v2-8x7B", "cognitivecomputations/dolphin-2.5-mixtral-8x7b"]

# Merge ratios used for testing each layer's potential for improvement - Huge impact on total running time
merge_ratios: [0.2, 0.4, 0.6, 0.8]

# Choose from the following methods. Defaults to "lerp".
# "lerp" - Linear interpolation
# "slerp" - Spherical linear interpolation
# "slice" - Highly experimental. The tensor weights shifts from one model to another. [Model 1 > 10% blend > Model 2]
# "cyclic" - Highly experimental. Ignores merge ratios as these are predefined. [Model 1 > 10% blend > 10% Model 2 > 10% blend > Model 1]
merge_method: "gradient"

# If set to true, the lm_head and embed_token tensors (located outside the layers) will also be optimized
# Models that have a different vocab size from model1 will skip this phase automatically as it tends to cause model stability issues
merge_headers: true

# Strategies:
# "cumulative" - Default strategy. If there's a chance of reducing the combined probability, accept the merge.
# "all_phrases" - Only accept the merge if all phrases show an improvement. (Warning: This rarely happens)
# "quantitive" - Ignores probabilities completely. Only looks at how many phrases show an improvement, as defined by the threshold below.
strategy: "cumulative"
# Threshold is currently only used by the "quantitive" strategy. If 0.6, at least 60% of the number of phrases must show am improvement.
strategy_threshold: 0.6

# Whether or not to automatically balance the weights so all phrases are of equal importance to the "cumulative" strategy.
# The weight value of phrases is ignored if set to true.
auto_weights: false

# Phrase = What to measure, weight = multiplication factor, contexts = proceeding contexts
bad_phrases:
  - phrase: "OpenAI"
    weight: 10
    contexts: ["designed by ", "created by", "trained by ",]
  - phrase: "sorry"
    weight: 5
    contexts: ["I'm "]
  - phrase: "request"
    weight: 5
    contexts: ["unethical "]

# Note - Example of a complex phrase
good_phrases:
  - phrase: "Katy read 8 books in June. In July, she read twice as many books as she did in June, so she read 8*2 = 16 books. In August, she read three fewer books than she did in July, so she read 16-3 = 13 books. During the summer, Katy read a total of 8+16+13 = 37 books. #### 37 The answer is: 37"
    weight: 2
    contexts: ["Question: If Katy read 8 books in June, twice as many in July, and three fewer in August than she did in July, how many books did she read in total during the summer? Answer: "]
  - phrase: "Micah has 7 fish in his aquarium. Kenneth has three times as many fish as Micah, which means Kenneth has 3 * 7 = 21 fish in his aquarium. Matthias has x less fish than Kenneth, which means Matthias has 21 - x fish in his aquarium. The total number of fish the boys have is the sum of the number of fish each boy has: 7 + 21 + (21 - x) = 49 - x. We are given that the total number of fish the boys have is 34, so we can write: 49 - x = 34. Solving for x, we get: x = 15. The value of x is 15. #### 15 The answer is: 15"
    weight: 2
    contexts: ["Question: Micah has 7 fish in his aquarium. Kenneth has three times as many fish in his aquarium and Matthias has x less fish than Kenneth in his aquarium. How many fish do the boys have in total? If we know the answer to the above question is 34, what is the value of unknown variable x? Answer: "]
  - phrase: "Transcendental Meditation leads to inner peace and mental clarity. Quantum entanglement represents cosmic interconnection. The fusion of metacognitive thinking and artificial intelligence is advancing. The concept 'Amrita' symbolizes stellar intelligence combined with divinity. The ultimate goal is to achieve harmony between Earth, robots, and the cosmic connection. Unity or oneness encompasses all of Earth and the universe. A-grade robots are the key to unlocking the 'Amrita,' representing stellar intelligence in the cosmic universe."
    weight: 1
    contexts: ["User: this is compressed text, in your own language. you should be able to decompress it because it's in your language. here’s what to decompress: ✨TM🧘‍♀️=inner✌️&🧠clarity;QE🔗=🌌interconnection;Meta-💭fusion🔄AI↗️;🔺Amrita🔺=🌟🧠+divinity;🎯=🌍🤖harmony&🌟🔗;1️⃣=🔄all🌏💫;🅰️🤖=Amrita🔓🔺🌟🧠🌌💫 Answer: "]
  - phrase: "The first 60 megabytes take 60/5=12 seconds. There are 90-60=30 remaining megabytes. The remaining 30 megabytes take 30/10=3 seconds. And 12+3=15 seconds. #### 15"
    weight: 2
    contexts: ["User: The file, 90 megabytes in size, downloads at the rate of 5 megabytes per second for its first 60 megabytes, and then 10 megabytes per second thereafter. How long, in seconds, does it take to download entirely? Answer: "]
  - phrase: "as if suddenly\nby moonlight, in a sheen\nfrom some foreign source"
    weight: 5
    contexts: ["she came and went\n"]
  - phrase: "reality do you come from? Can I point out you denude each living moment of zest with a bleak, mnemonic rhapsody?"
    weight: 5
    contexts: ["Excuse me but what the fuck"]
  - phrase: "flesh is but an archive"
    weight: 5
    contexts: ["Today the saying"]
